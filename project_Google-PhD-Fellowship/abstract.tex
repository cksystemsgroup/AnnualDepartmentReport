{\bf{Scalable Software Systems through Relaxed Data Structures}}

This project is funded by a Google PhD fellowship awarded to Michael Lippautz who is a PhD student in the Computational Systems Group.

Parallel processing units (multiple processors with multiple cores) are ubiquitous these days. They can be found in regular desktop machines, big servers, and even in small computing devices such as smart phones. One of the principles for communication between software processes running on those processing units is the use of concurrent data structures (represented on shared-memory), e.g. concurrent First-In-First-Out (FIFO) queues and concurrent stacks. Communication through concurrent data structures, however, limits overall application performance. Hence, it is crucial to provide concurrent data structures that perform fast and scale well, i.e., on which the number of potential operations per second ideally scales linearly with the number of parallel processing units.

The problem is that synchronization (which cannot be parallelized) is necessary to guarantee certain data structure semantics. For example, take a FIFO queue where multiple processes enqueue and dequeue elements concurrently. Providing a strict order inherently requires synchronization as concurrent operations need to determine which operation is performed first. This is even independent of the internal representation of data. The consequence is that programmers avoid such scenarios at all cost, for example by implementing complex cache and distribution logics. However, there is another way out of this quandary: using data structures with relaxed semantics, e.g. by allowing elements to overtake each other in a FIFO queue. The challenge here is to design algorithms that trade off semantics for performance/scalability and to quantify their behavior.

At POPL 2013 we showed that such relaxed data structures perform and scale better than existing implementations, may even provide better determinism under certain circumstances, and can be used in actual systems software such as the scalloc memory allocator developed by us. Scalloc is particularly interesting, because the use of scalable data structures not only increases performance and reduces memory consumption, but also allows a simpler design based on global data structures. Hence, we design general purpose concurrent data structures that will eventually replace custom distribution and cache logics and lead to systems that perform and scale better than currently existing ones.